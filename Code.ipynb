{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# File system manangement\nimport os\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \n# matplotlib and seaborn for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nimport pandas as pd \nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read Customer usage file\nCustomer_usage = pd.read_csv('../input/Customer_usage.txt',delimiter='\\t', decimal=',')\nprint('Customer usage shape: ', Customer_usage.shape)\nCustomer_usage.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read Customer churn file\nCustomer_churn = pd.read_csv('../input/Customer_churn.txt',delimiter='\\t')\nprint('Customer churn shape: ', Customer_churn.shape)\nCustomer_churn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examining missing values in customer usage\ntotal = Customer_usage.isnull().sum().sort_values(ascending=False)\npercent = round((Customer_usage.isnull().sum().sort_values(ascending=False)/Customer_usage.shape[0])*100,2)\nMissing_Values =pd.concat([total,percent], axis = 1,keys= ['Missing Values', '% of Total Values'])\nMissing_Values.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examining missing Value in customer churn\ntotal = Customer_churn.isnull().sum().sort_values(ascending=False)\npercent = round((Customer_churn.isnull().sum().sort_values(ascending=False)/Customer_churn.shape[0])*100,2)\nMissing_Values =pd.concat([total,percent], axis = 1,keys= ['Missing Values', '% of Total Values'])\nMissing_Values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of each type of column\nCustomer_usage.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Descriptive statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a fuction to set the bar width\nwidthbars  = [0.3, 0.3, 0.3]\ndef set_bar_width(ax):\n    for p, newwidth in zip(ax.patches, widthbars):\n        x = p.get_x()\n        width = p.get_width()\n        centre = x + width/2.\n        p.set_x(centre - newwidth/2.)\n        p.set_width(newwidth)\n\n# Define a function to show values on bar\ndef show_value_on_bar(ax):        \n    for p in ax.patches:\n        x = p.get_x() + p.get_width() / 2\n        y = p.get_y() + p.get_height()\n        value = '{:.2f}'.format(p.get_height())\n        ax.text(x, y, value, color='black',fontsize=15,ha=\"center\")\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute totals per month/year on all numeic features except id columns and user_lifetime\n\ndf_stat= Customer_usage.drop(columns = ['user_account_id','user_intake'])\ndf_stat_sum_per_period = df_stat.drop(columns = ['user_lifetime']).groupby(['year','month'], as_index = False).agg(['sum']).reset_index()\ndf_stat_sum_per_period['period']=df_stat_sum_per_period['month'].astype(str)+'/'+df_stat_sum_per_period['year'].astype(str)\ndf_stat_sum_per_period.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# barplots\n\nimport matplotlib.pyplot as plt\ni=0\nplt.figure(figsize=(25,120))\nfor col in df_stat_sum_per_period.iloc[:,2:-1]:\n    if col[0] != 'user_lifetime':\n        plt.subplot(20,3,i+1)\n        ax= sns.barplot(x='period',y= col ,data=df_stat_sum_per_period,palette=\"rocket\",linewidth= 2  )\n        plt.title(\"Total of \"+ col[0] + \" per period\", fontsize = 15)\n        plt.ylabel(\"Total of \"+ col[0], fontsize = 10)\n        plt.xlabel(\"Period\",fontsize = 10)\n        set_bar_width(ax)\n        show_value_on_bar(ax)       \n        i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute % of customer per month on the 4 binary columns\ni=0\nplt.figure(figsize=(20,10))\nfor col in Customer_usage.iloc[:,8:12]:\n    plt.subplot(2,2,i+1)\n    ax= sns.barplot(x='month',y= col ,data=Customer_usage,palette=\"rocket\",linewidth=2 )\n    plt.title(\"% of \"+ col + \" per month\", fontsize = 15)\n    plt.ylabel(\"% of \"+ col, fontsize = 12)\n    plt.xlabel(\"Month\",fontsize = 12)\n    set_bar_width(ax)\n    show_value_on_bar(ax)       \n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute totals over the whole dataset except id columns and user_lifetime\n\ndf_stat_sum = df_stat.drop(columns = ['user_lifetime','year','month']).agg(['sum'])\ndf_stat_sum.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.drop(columns = ['user_lifetime','year','month']).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns distribution\nimport matplotlib.pyplot as plt\ni=0\nplt.figure(figsize=(25,120))\nfor col in df_stat.iloc[:,2:]:\n    plt.subplot(21,3,i+1)\n    plt.hist(df_stat[col],bins=25)\n    plt.title(col, fontsize = 20)\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Churn Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preparing the dataset for the churn prediction model (one row per cusstomer)\n# Group by the client id, calculate aggregation statistics\ncustomer_usage_agg_by_id= Customer_usage.drop(columns = ['year','month','user_intake']).groupby('user_account_id', as_index = False).agg(['mean']).reset_index()\ncustomer_usage_agg_by_id.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customer_usage_agg_by_id.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of column names\ncolumns = ['user_account_id']\n\n# Iterate through the variables names\nfor var in customer_usage_agg_by_id.columns.levels[0]:\n    # Skip the id name\n    if var != 'user_account_id':\n        \n        # Iterate through the stat names\n        for stat in customer_usage_agg_by_id.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('%s_%s' % (stat, var))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign the list of columns names as the dataframe column names\ncustomer_usage_agg_by_id.columns = columns\ncustomer_usage_agg_by_id.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Consider max user_lifetime by customer instead of the mean\nmax_userlifetime_by_id= Customer_usage[['user_account_id','user_lifetime']].groupby('user_account_id', as_index = False).agg(['max']).reset_index()\nmax_userlifetime_by_id.columns =['user_account_id','max_user_lifetime']\ncustomer_usage_agg_by_id = customer_usage_agg_by_id.merge(max_userlifetime_by_id, on = 'user_account_id', how = 'inner')\ncustomer_usage_agg_by_id.drop(columns = ['mean_user_lifetime'], inplace =True)\ncustomer_usage_agg_by_id.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge with the target column\ndata_by_customer = customer_usage_agg_by_id.merge(Customer_churn, on = 'user_account_id', how = 'left')\ndata_by_customer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_by_customer.drop(columns = ['year','month','user_account_id'],inplace =True)\ndata_by_customer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_by_customer.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = data_by_customer.iloc[:,:-1]\ntarget = data_by_customer['churn']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examining the distribution of the target value\nf,ax=plt.subplots(1,2,figsize=(18,8))\ndata_by_customer['churn'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n\nax[0].set_title('Churn')\nax[0].set_ylabel('')\nsns.countplot('churn',data = data_by_customer,ax=ax[1])  \nshow_value_on_bar(ax[1])    \nax[1].set_title('Churn')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this information, we see this is an imbalanced classification problem. There are far more customers that didn't churn than customers that churn. "},{"metadata":{"trusted":true},"cell_type":"code","source":"features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns distribution\ni=0\nplt.figure(figsize=(25,120))\nfor col in features:\n    plt.subplot(21,3,i+1)\n    plt.hist(features[col],bins=25)\n    plt.title(col, fontsize = 20)\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute correlations among features (heatmap)\nmask = np.zeros_like(features.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize = (15,12))\nsns.heatmap(features.corr(), \n            annot=False,\n            mask = mask,\n            cmap = 'RdBu_r',# 'YlGnBu',\n            linewidths=0.1, \n            linecolor='white',\n            vmax = .9,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute correlatons with the target column\ncorrelations = data_by_customer.corr()['churn'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kde plots: Distribution of the most correlated features to the target\n\ntemp_list= list(correlations.abs().sort_values(ascending=True).tail(12).index)\ntemp_list.remove('churn')\nmost_corr = data_by_customer.loc[:, temp_list]\n# temp.head()\n\ni=0\nplt.figure(figsize=(25,25))\nfor col in most_corr:\n    plt.subplot(4,3,i+1)\n    sns.kdeplot(data_by_customer.loc[data_by_customer['churn'] == 0,col] ,color='gray',shade=True, label = 'churn == 0')\n    sns.kdeplot(data_by_customer.loc[data_by_customer['churn'] == 1,col] ,color='g',shade=True, label = 'churn == 1')\n    plt.title(col+ ' Distribution', fontsize = 15)\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Catplot function \ndef cat_plot(feature, cut=12):\n    temp = data_by_customer.loc[:, [feature, \"churn\"]]\n    temp[feature + \"_binned\"] = pd.qcut(temp[feature], cut, duplicates=\"drop\")\n    ax = sns.catplot(\n        x=\"churn\",\n        y=feature + \"_binned\",\n        data=temp,\n        kind=\"bar\",\n        height=5,\n        aspect=2.7,\n    )\n    \n    plt.xlabel('Churn rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cat plot of the most correlated features to the target\nfor col in most_corr: \n    cat_plot(col, cut=9)\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Downsampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.utils import resample\n# # Class count\n# count_majority, count_minorty = data_by_customer.target.value_counts()\n# # Divide by class\n# df_majority = data_by_customer[data_by_customer['churn'] == 0]\n# df_minority  = data_by_customer[data_by_customer['churn'] == 1]\n# \n# df_majority_downsampled = resample(df_majority, \n#                                  replace=False,    # sample without replacement\n#                                  n_samples=count_minorty,     # to match minority class\n#                                  random_state=123)\n# df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n# print('Random down-sampling:')\n# print(df_downsampled.churn.value_counts())\n# \n# df_downsampled.churn.value_counts().plot(kind='bar', title='Count (churn)');\n# features = df_downsampled.iloc[:,:-1]\n# target = df_downsampled['churn']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data into train set and test set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(features,target,test_size=0.2,stratify=target)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling Data\nfrom sklearn.preprocessing import StandardScaler\nScaler = StandardScaler().fit(X_train)\nX_train_Scaled = Scaler.transform(X_train)\nX_test_Scaled = Scaler.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Prediction Models: Training and Evaluation "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a roc curve plot function\nfrom sklearn.metrics import roc_curve,auc\ndef Plot_Roc_Curve(label):\n    fpr,tpr, thres = roc_curve(y_test,y_pred)\n    fpr_train,tpr_train, thres = roc_curve(y_train,y_pred_train)\n    roc_auc= auc(fpr,tpr)\n    roc_auc_train= auc(fpr_train,tpr_train)\n    plt.figure()\n    plt.plot(fpr, tpr, label= 'ROC curve on testing set (area = %0.2f)' % roc_auc, linewidth= 4)\n    plt.plot(fpr_train, tpr_train, label= 'ROC curve on traing set (area = %0.2f)' % roc_auc_train, linewidth= 4)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 4)\n    plt.xlim([0.0,1.0])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate', fontsize = 18)\n    plt.ylabel('True Positive Rate', fontsize = 18)\n    plt.title('ROC curve/'+ label, fontsize= 18)\n    plt.legend(loc=\"lower right\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a precision recall plot function\nfrom sklearn.metrics import precision_recall_curve\ndef Plot_Precision_Recall_Curve(label):\n    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n    precision_train, recall_train, _ = precision_recall_curve(y_train, y_pred_train)\n    PR_AUC = auc(recall, precision)\n    PR_AUC_train = auc(recall_train, precision_train)\n    plt.figure()\n    plt.plot(recall, precision, label='PR curve on test set (area = %0.2f)' % PR_AUC, linewidth=4)\n    plt.plot(recall_train, precision_train, label='PR curve on train set (area = %0.2f)' % PR_AUC_train, linewidth=4)\n    plt.xlabel('Recall', fontsize=18)\n    plt.ylabel('Precision', fontsize=18)\n    plt.title('Precision Recall Curve/'+label,fontsize=18)\n    plt.legend(loc=\"lower right\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df):\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:20]))), \n            df['importance_normalized'].head(20), \n            align = 'center', edgecolor = 'k')\n  \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:20]))))\n    ax.set_yticklabels(df['feature'].head(20))\n   \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n   \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\nGNB = GaussianNB()\n\nGNB.fit(X_train_Scaled, y_train)\n\n\ny_pred= GNB.predict_proba(X_test_Scaled)[:, 1]\ny_pred_train= GNB.predict_proba(X_train_Scaled)[:, 1]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_Roc_Curve('Gaussian Naive Bayes')\nPlot_Precision_Recall_Curve('Gaussian Naive Bayes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nLG = LogisticRegression()\n\nparams = {'C': np.logspace(-3,3,10),'penalty':['l1','l2'] }\nLG_CV = RandomizedSearchCV(LG,params,n_iter=20,cv=5,verbose = False,scoring='roc_auc', n_jobs=-1)\nLG_CV.fit(X_train_Scaled, y_train)\nprint('Best params: ')\nprint(LG_CV.best_params_) \nprint('Best score: ')\nprint(LG_CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= LG_CV.predict_proba(X_test_Scaled)[:, 1]\ny_pred_train= LG_CV.predict_proba(X_train_Scaled)[:, 1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_Roc_Curve('Logistic Regression')\nPlot_Precision_Recall_Curve('Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  K-Nearest Neighbor classifier(KNN)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nKNN = KNeighborsClassifier(metric='minkowski', p=2)\nk_range = range(1,31)\nweights_options=['uniform','distance']\nparams = {'n_neighbors':k_range, 'weights':weights_options}\n\nKNN_CV = RandomizedSearchCV(KNN,params,n_iter=20,cv=5,verbose = False,scoring='roc_auc', n_jobs=-1)\n## Fitting the model. \nKNN_CV.fit(X_train_Scaled, y_train)\nprint('Best params: ')\nprint(KNN_CV.best_params_) \nprint('Best score: ')\nprint(KNN_CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= KNN_CV.predict_proba(X_test_Scaled)[:, 1]\ny_pred_train= KNN_CV.predict_proba(X_train_Scaled)[:, 1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_Roc_Curve('KNN')\nPlot_Precision_Recall_Curve('KNN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nSVC = SVC()\n\nparams={'C': np.logspace(-3,3,10),'gamma': np.logspace(-2,2,5), 'kernel':['linear','rbf']}\nSVC_CV=RandomizedSearchCV(SVC, params,n_iter=10,cv=3,verbose = False,scoring='roc_auc', n_jobs=-1)\n\nSVC_CV.fit(X_train_Scaled, y_train)\n\nprint('Best params: ')\nprint(SVC_CV.best_params_) \nprint('Best score: ')\nprint(SVC_CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= SVC_CV.predict_proba(X_test_Scaled)[:, 1]\ny_pred_train= SVC_CV.predict_proba(X_train_Scaled)[:, 1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_Roc_Curve('Support Vector Machine')\nPlot_Precision_Recall_Curve('Support Vector Machine')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRFC = RandomForestClassifier(random_state=42)\nparams = {'n_estimators':[50,100,200],\n          'max_features' : ['sqrt','log2'],\n          'criterion': ['entropy', 'gini'],\n          'min_samples_split': [2, 3, 5],\n          'min_samples_leaf': [3, 4, 5, 6, 7],\n          'max_depth': [7,8,9,10,11]\n         }\nRFC_CV = RandomizedSearchCV (RFC,params,n_iter=20,cv=5,verbose = False,scoring='roc_auc', n_jobs=-1)\nRFC_CV.fit(X_train_Scaled,y_train)\nprint('Best params: ')\nprint(RFC_CV.best_params_) \nprint('Best score: ')\nprint(RFC_CV.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=RFC_CV.predict_proba(X_test_Scaled)[:, 1]\ny_pred_train=RFC_CV.predict_proba(X_train_Scaled)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_Roc_Curve('Random Forest Classifier')\nPlot_Precision_Recall_Curve('Random Forest Classifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. Given the correlations we saw in the exploratory data analysis, we should expect that the most important features are the most correlated to the target. We may use these feature importances as a method of dimensionality reduction in future work."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract feature importances\nFeatures = list(X_train.columns)\nfeature_importance_values = RFC_CV.best_estimator_.feature_importances_\nfeature_importances_RFC = pd.DataFrame({'feature': Features, 'importance': feature_importance_values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the feature importances for the default features\nfeature_importances_RFC_sorted = plot_feature_importances(feature_importances_RFC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nGBC= GradientBoostingClassifier()\n\nparams = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"n_estimators\":[100,150,200]\n    }\nGBC_CV = RandomizedSearchCV(GBC,params,cv=5,n_iter=10,verbose = False,scoring='roc_auc', n_jobs=-1)\nGBC_CV.fit(X_train_Scaled,y_train)\nprint('Best params: ')\nprint(GBC_CV.best_params_) \nprint('Best score: ')\nprint(GBC_CV.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= GBC_CV.predict_proba(X_test_Scaled)[:, 1]\ny_pred_train= GBC_CV.predict_proba(X_train_Scaled)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_Roc_Curve('Gradient Boosting Classifier')\nPlot_Precision_Recall_Curve('Gradient Boosting Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract feature importances\nfeature_importance_values = GBC_CV.best_estimator_.feature_importances_\nfeature_importances_GBC = pd.DataFrame({'feature': Features, 'importance': feature_importance_values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the feature importances for the default features\nfeature_importances_GBC_sorted = plot_feature_importances(feature_importances_GBC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier\nXGBC = XGBClassifier(objective = 'binary:logistic')\n\n\nparams = {\n     'silent': [False],\n     'max_depth': [2, 3, 4, 5],\n     'learning_rate': [0.001, 0.01, 0.1, 0.15],\n     'subsample': [0.7, 0.8, 0.9, 1.0],\n     'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n     'colsample_bylevel': [0.7, 0.8, 0.9, 1.0],\n     'min_child_weight': [0.5, 1.0, 3.0],\n     'gamma': [0, 0.25, 0.5, 1.0],\n     'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n     'n_estimators': [50, 100, 150],\n     'scale_pos_weight': [1, 1.5, 2],\n     'max_delta_step': [1, 2, 3]\n }\n \nXGBC_CV = RandomizedSearchCV(XGBC,params,n_iter=20,cv=5,verbose = False,scoring='roc_auc', n_jobs=-1)\nXGBC_CV.fit(X_train_Scaled,y_train)\nprint('Best params: ')\nprint(XGBC_CV.best_params_) \nprint('Best score: ')\nprint(XGBC_CV.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred= XGBC_CV.predict_proba(X_test_Scaled)[:, 1]\ny_pred_train= XGBC_CV.predict_proba(X_train_Scaled)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Plot_Roc_Curve('Extreme Gradient Boosting Classifier')\nPlot_Precision_Recall_Curve('Extreme Gradient Boosting Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract feature importances\nfeature_importance_values = XGBC_CV.best_estimator_.feature_importances_\nfeature_importances_XGBC = pd.DataFrame({'feature': Features, 'importance': feature_importance_values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the feature importances for the default features\nfeature_importances_XGBC_sorted = plot_feature_importances(feature_importances_XGBC)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Customer Segmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling Features\nfrom sklearn.preprocessing import StandardScaler\nfeatures_scaled = StandardScaler().fit_transform(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose the number of cluster based on silhouette score\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nscore = []\nx = list(range(2, 9))\n\nfor n_clusters in x:\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=40)\n    kmeans.fit(features_scaled)\n    clusters = kmeans.predict(features_scaled)\n    silhouette_avg = silhouette_score(features_scaled, clusters)\n    score.append(silhouette_avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the evolution of the silhouette score\n\nplt.figure(figsize=(20,16))\nplt.plot(x, score)\nplt.title(\"Evolution of the Silhouette Score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nn_clusters = 3\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30, random_state=0)\nproj = kmeans.fit_transform(features_scaled)\nclusters = kmeans.predict(features_scaled)\n\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"Visualising the clusters\", fontsize=\"20\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of the clustering with TSNE\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2)\nproj = tsne.fit_transform(features_scaled)\n\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"Visualization of the clustering with TSNE\", fontsize=\"20\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot The number of customers by cluster\nplt.figure(figsize = (20,8))\nn, bins, patches = plt.hist(clusters, bins=3)\nplt.xlabel(\"Cluster\")\nplt.title(\"Number of customers per cluster\")\nplt.xticks([rect.get_x()+ rect.get_width() / 2 for rect in patches], [\"Cluster {}\".format(x) for x in range(3)])\nfor rect in patches:\n    y_value = rect.get_height()\n    x_value = rect.get_x() + rect.get_width() / 2\n    space = 5\n    va = 'bottom'\n    label = str(int(y_value))\n    plt.annotate(label,(x_value, y_value),xytext=(0, space),textcoords=\"offset points\",ha='center',va=va)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the mostimportant features by cluster\nfeatures['clusters']=clusters\nfeatures['max_user_lifetime']=features['max_user_lifetime']/365\n\ntemp_list = list(feature_importances_RFC_sorted.iloc[:20, 1])\ntemp_list.append(\"clusters\")\n\ntemp = features.loc[:, temp_list]\ntemp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ni=0\nplt.figure(figsize=(40,120))\nfor col in temp.iloc[:,:-1]:\n    plt.subplot((temp.shape[1]-1)/2+1,2,i+1)\n    ax= sns.barplot(x='clusters',y= col ,data=temp,palette=\"rocket\",linewidth= 2  )\n    plt.title(\"Distribution of \"+ col + \" per cluster\", fontsize = 25)\n    plt.ylabel(col, fontsize = 20)\n    plt.xlabel(\"Clusters\",fontsize = 20)\n    show_value_on_bar(ax)       \n    i=i+1\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}